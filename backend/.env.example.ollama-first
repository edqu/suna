# Ollama-First Configuration (FREE, LOCAL, PRIVATE)
# Copy these lines to your backend/.env file

# ============================================================================
# OLLAMA-FIRST WITH GEMINI FALLBACK (RECOMMENDED)
# ============================================================================

# Use FREE local Ollama for browser vision (instead of expensive Gemini)
BROWSER_VISION_MODEL=ollama/qwen2-vl

# Ollama server URL (default for local installation)
OLLAMA_API_BASE=http://localhost:11434

# Enable browser-first mode for web searches
BROWSER_DEFAULT_FOR_WEB_SEARCH=true

# OPTIONAL: Gemini as fallback (only used if Ollama fails)
# Get free key: https://aistudio.google.com/app/apikey
GEMINI_API_KEY=your-gemini-key-as-fallback

# ============================================================================
# SETUP STEPS
# ============================================================================

# 1. Install Ollama:
#    https://ollama.com/download

# 2. Pull a vision model:
#    ollama pull qwen2-vl       (Recommended - 4GB)
#    # or
#    ollama pull qwen3-vl       (Newer - 5GB)
#    # or
#    ollama pull llava          (Smaller - 3GB)

# 3. Verify installation:
#    ollama list
#    # Should show your vision model

# 4. Restart backend:
#    python start.py

# 5. Look for logs:
#    Using vision model: ollama/qwen2-vl
#    Using Ollama vision model at http://localhost:11434
#    ✅ Stagehand API server initialized successfully

# ============================================================================
# HOW IT WORKS
# ============================================================================

# Priority 1: Try Ollama vision model (FREE!)
#   - Completely local
#   - 100% private
#   - No API costs
#   - No rate limits
#
# Priority 2: Fallback to Gemini (if Ollama fails)
#   - Only when Ollama unavailable
#   - Uses GEMINI_API_KEY
#   - Cloud-based
#
# Priority 3: Fallback to API search (if both fail)
#   - DuckDuckGo API
#   - Free and always available
#   - No vision capability

# ============================================================================
# COST COMPARISON
# ============================================================================

# With Ollama-First:
#   - Browser vision: $0/month (Ollama)
#   - Gemini usage: ~$0/month (rarely used)
#   - Total: $0/month ✅

# With Gemini Only (old way):
#   - Browser vision: Gemini API costs
#   - Within free tier but limited
#   - Total: $0-$20/month depending on usage

# Savings: 100% of vision API costs!

# ============================================================================
# VERIFICATION
# ============================================================================

# After setup, verify:
# - ollama list (should show qwen2-vl or your chosen model)
# - Backend logs show "Using vision model: ollama/..."
# - Test: "Navigate to example.com and describe what you see"
# - Logs should NOT show Gemini API calls (all handled by Ollama)

# ============================================================================
# TROUBLESHOOTING
# ============================================================================

# Ollama not found:
#   - Install from https://ollama.com/download
#   - Verify running: ollama list

# Model not pulled:
#   - Run: ollama pull qwen2-vl

# Still using Gemini:
#   - Check BROWSER_VISION_MODEL is set correctly
#   - Restart backend
#   - Check logs for "Using vision model: ollama/..."

# Connection refused:
#   - Check Ollama is running
#   - Verify OLLAMA_API_BASE=http://localhost:11434
#   - Test: curl http://localhost:11434/api/tags
